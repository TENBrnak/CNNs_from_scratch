{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:29.012267Z",
     "start_time": "2024-04-02T19:27:28.635476Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notes for users\n",
    "## Installation\n",
    "- all packages required are listed above, namely numpy, scipy and matplotlib (visualization - optional)\n",
    "- I recommend running in a Jupyter notebook\n",
    "- pulling the entire repository is also required to get all the accompanying files such as model saves and datasets\n",
    "## User Guide\n",
    "- run this notebook mostly sequentially, I recommend skipping the training stages as they take quite long and using the saved models instead\n",
    "- this notebook is intended for people who are able to read code, it is a proof-of-concept and in no way, shape or form intended for any actual use\n",
    "# Inspiration/Source\n",
    "- heavy inspiration for this work has been taken from the Neural Networks from Scratch by Harrison Kinsley and Daniel Kukiela available at https://nnfs.io/, code for the LayerDense, ActivationReLU, ActivationSoftmaxLossCategoricalCrossentropy and OptimizerSGD classes were taken directly from the book with some but often only limited modifications\n",
    "- the LayerConvolutional, LayerMaxPooling, LayerFlatten and Network classes were written entirely by me"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    return np.eye(num_classes)[y]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:29.626300Z",
     "start_time": "2024-04-02T19:27:29.624470Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read mnist data\n",
    "def read_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "        assert int.from_bytes(data[:4], byteorder='big') == 2051\n",
    "        n_images = int.from_bytes(data[4:8], byteorder='big')\n",
    "        n_rows = int.from_bytes(data[8:12], byteorder='big')\n",
    "        n_cols = int.from_bytes(data[12:16], byteorder='big')\n",
    "        images = np.frombuffer(data, dtype=np.uint8, offset=16).reshape(n_images, n_rows, n_cols)\n",
    "        return images, n_images\n",
    "\n",
    "\n",
    "def read_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = f.read()\n",
    "        assert int.from_bytes(data[:4], byteorder='big') == 2049\n",
    "        labels = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "        return labels\n",
    "\n",
    "\n",
    "x, num_inputs = read_mnist_images('../mnist/train-images.idx3-ubyte')\n",
    "x = x.reshape((-1, 1, 28, 28))\n",
    "x = x / 255.\n",
    "y = one_hot_encode(read_mnist_labels('../mnist/train-labels.idx1-ubyte'), 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:50.801732Z",
     "start_time": "2024-04-02T19:27:50.724724Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:54.342813Z",
     "start_time": "2024-04-02T19:27:54.337230Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerDense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        rng = np.random.default_rng()\n",
    "        variance = np.sqrt(6/(n_inputs + n_neurons))\n",
    "        self.weights = (rng.standard_normal(size=(n_inputs, n_neurons))) * variance # xavier initialization from tensorflow\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item == 'weights':\n",
    "            return self.weights\n",
    "        elif item == 'biases':\n",
    "            return self.biases\n",
    "        elif item == 'n_inputs':\n",
    "            return self.n_inputs\n",
    "        elif item == 'n_neurons':\n",
    "            return self.n_neurons\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key == 'weights':\n",
    "            self.weights = value\n",
    "        elif key == 'biases':\n",
    "            self.biases = value\n",
    "        elif key == 'n_neurons':\n",
    "            self.n_neurons = value\n",
    "        elif key == 'n_inputs':\n",
    "            self.n_inputs = value\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        self.dweights = np.dot(self.inputs.T, d_values) / len(self.inputs)\n",
    "        self.dbiases = np.sum(d_values, axis=0, keepdims=True) / len(self.inputs)\n",
    "        self.dinputs = np.dot(d_values, self.weights.T) / len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LayerConvolutional:\n",
    "    def __init__(self, num_kernels, num_channels, kernel_size):\n",
    "        rng = np.random.default_rng()\n",
    "        variance = np.sqrt(6/(2*num_kernels*num_channels*kernel_size*kernel_size))\n",
    "        self.weights = (2*rng.standard_normal(size=(num_kernels, num_channels, kernel_size, kernel_size))) * variance\n",
    "        self.biases = np.zeros(num_kernels)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_kernels = num_kernels\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if item == 'weights':\n",
    "            return self.weights\n",
    "        elif item == 'biases':\n",
    "            return self.biases\n",
    "        elif item == 'num_kernels':\n",
    "            return self.num_kernels\n",
    "        elif item == 'num_channels':\n",
    "            return self.num_channels\n",
    "        elif item == 'kernel_size':\n",
    "            return self.kernel_size\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key == 'weights':\n",
    "            self.weights = value\n",
    "        elif key == 'biases':\n",
    "            self.biases = value\n",
    "        elif key == 'num_kernels':\n",
    "            self.num_kernels = value\n",
    "        elif key == 'num_channels':\n",
    "            self.num_channels = value\n",
    "        elif key == 'kernel_size':\n",
    "            self.kernel_size = value\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if hasattr(self, 'output'):\n",
    "            delattr(self, 'output')\n",
    "        self.pad_size_rows = inputs.shape[-2] - (inputs.shape[-2] - self.kernel_size + 1)\n",
    "        self.pad_size_cols = inputs.shape[-1] - (inputs.shape[-1] - self.kernel_size + 1)\n",
    "        self.inputs = inputs\n",
    "        self.padded_inputs = np.pad(inputs, ((0, 0), (0, 0), (0, self.pad_size_rows), (0, self.pad_size_cols)))\n",
    "        if self.num_channels != self.inputs.shape[1]:\n",
    "            raise Exception('Error: number of filter and image channels does not match.')\n",
    "        for kernel, bias in zip(self.weights, self.biases):\n",
    "            if not hasattr(self, 'output'):\n",
    "                self.output = sp.signal.convolve(self.padded_inputs, [kernel], mode='valid') + bias\n",
    "            else:\n",
    "                self.output = np.append(self.output, sp.signal.convolve(self.padded_inputs, [kernel], mode='valid') + bias, axis=1)\n",
    "    \n",
    "    def backward(self, d_values):\n",
    "        self.dbiases = np.zeros_like(self.biases)\n",
    "        self.dinputs = np.zeros_like(self.inputs)\n",
    "        if hasattr(self, 'dweights'):\n",
    "            delattr(self, 'dweights')\n",
    "        for kernel_id in range(self.num_kernels):\n",
    "            kernel = self.weights[kernel_id]\n",
    "            rotated_kernel = np.rot90(kernel, k=2, axes=(1, 2))\n",
    "            per_kernel_dvalues = d_values[:, kernel_id:(kernel_id+1), :, :]\n",
    "            for channel_id in range(self.num_channels):\n",
    "                channel_rot_kernel = rotated_kernel[channel_id]\n",
    "                channel_inputs = self.padded_inputs[:, channel_id:channel_id+1, :, :]\n",
    "                if channel_id == 0:\n",
    "                    cur_dfilter = sp.signal.convolve(channel_inputs, per_kernel_dvalues, mode='valid').reshape((-1, 1, self.kernel_size, self.kernel_size))\n",
    "                else:\n",
    "                    cur_dfilter = np.append(cur_dfilter, sp.signal.convolve(channel_inputs, per_kernel_dvalues, mode='valid').reshape((-1, 1, self.kernel_size, self.kernel_size)), axis=1)\n",
    "                self.dinputs[:, channel_id:channel_id+1, :, :] += sp.signal.convolve(per_kernel_dvalues, [[channel_rot_kernel]], mode='full')[:, :, (self.pad_size_rows//2):-(self.pad_size_rows//2), (self.pad_size_cols//2):-(self.pad_size_cols//2)]\n",
    "            if not hasattr(self, 'dweights'):\n",
    "                self.dweights = cur_dfilter\n",
    "            else:\n",
    "                self.dweights = np.append(self.dweights, cur_dfilter, axis=0)\n",
    "            self.dbiases[kernel_id] += np.sum(per_kernel_dvalues)\n",
    "            \n",
    "            self.dbiases /= len(self.inputs)\n",
    "            self.dinputs /= len(self.inputs)\n",
    "            self.dweights /= len(self.inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:56.703835Z",
     "start_time": "2024-04-02T19:27:56.696445Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LayerMaxPooling:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        if item == 'stride':\n",
    "            return self.stride\n",
    "        elif item == 'kernel_size':\n",
    "            return self.kernel_size\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key == 'stride':\n",
    "            self.stride = value\n",
    "        elif key == 'kernel_size':\n",
    "            self.kernel_size = value\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        pad_cols = self.inputs.shape[-1] % self.stride if self.inputs.shape[-1] % self.stride != 0 else 0\n",
    "        pad_rows = self.inputs.shape[-2] % self.stride if self.inputs.shape[-2] % self.stride != 0 else 0\n",
    "        self.padded_inputs = np.pad(inputs, ((0, 0), (0, 0), (0, pad_rows), (0, pad_cols))) if pad_cols or pad_rows else inputs\n",
    "        self.windowed_padded_inputs = np.lib.stride_tricks.sliding_window_view(self.padded_inputs, (self.kernel_size, self.kernel_size), axis=(2, 3))[:, :, ::self.stride, ::self.stride]\n",
    "        self.output = self.windowed_padded_inputs.max(axis=(4, 5))\n",
    "        \n",
    "    def backward(self, d_values):\n",
    "        self.dinputs = np.zeros_like(self.inputs)\n",
    "        samples, channels, rows, cols, window_rows, window_cols = self.windowed_padded_inputs.shape\n",
    "        for sample in range(samples):\n",
    "            for channel_id in range(channels):\n",
    "                for row in range(rows):\n",
    "                    for col in range(cols):\n",
    "                        window = self.windowed_padded_inputs[sample, channel_id, row, col]\n",
    "                        max_id = np.argmax(window)\n",
    "                        row_placement_in_block = max_id // window_cols\n",
    "                        col_placement_in_block = max_id % window_cols\n",
    "                        self.dinputs[sample, channel_id, window_rows*row + row_placement_in_block, window_cols*col + col_placement_in_block] = d_values[sample, channel_id, row, col]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:58.162485Z",
     "start_time": "2024-04-02T19:27:58.157931Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LayerFlatten:\n",
    "    def forward(self, inputs):\n",
    "        self.n_samples, self.n_channels, self.num_rows, self.num_cols = inputs.shape\n",
    "        self.output = inputs.reshape((self.n_samples, self.n_channels * self.num_rows * self.num_cols))\n",
    "    \n",
    "    def backward(self, d_values):\n",
    "        self.dinputs = d_values.reshape((self.n_samples, self.n_channels, self.num_rows, self.num_cols))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:30:22.657767Z",
     "start_time": "2024-04-02T20:30:22.654711Z"
    }
   },
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:59.098425Z",
     "start_time": "2024-04-02T19:27:59.096440Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActivationReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, d_values):\n",
    "        self.dinputs = d_values.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:27:59.660332Z",
     "start_time": "2024-04-02T19:27:59.657937Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActivationSoftmaxLossCategoricalCrossentropy:\n",
    "    def forward(self, inputs, correct_labels):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # -np.max(...) for numerical stability with big number\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        predictions = np.clip(self.output, 1e-7, 1 - 1e-7)\n",
    "        loss = np.sum(-np.log(predictions) * correct_labels)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dvalues, correct_labels):\n",
    "        self.dinputs = dvalues - correct_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:28:00.210931Z",
     "start_time": "2024-04-02T19:28:00.206540Z"
    }
   },
   "outputs": [],
   "source": [
    "class OptimizerSGD:\n",
    "    def __init__(self, learning_rate=1.0, decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        if key == 'learning_rate':\n",
    "            self.learning_rate = value\n",
    "        elif key == 'decay':\n",
    "            self.decay = value\n",
    "        elif key == 'current_learning_rate':\n",
    "            self.current_learning_rate = value\n",
    "        elif key == 'iterations':\n",
    "            self.iterations = value\n",
    "        elif key == 'momentum':\n",
    "            self.momentum = value\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if item == 'learning_rate':\n",
    "            return self.learning_rate\n",
    "        elif item == 'decay':\n",
    "            return self.decay\n",
    "        elif item == 'current_learning_rate':\n",
    "            return self.current_learning_rate\n",
    "        elif item == 'iterations':\n",
    "            return self.iterations\n",
    "        elif item == 'momentum':\n",
    "            return self.momentum\n",
    "\n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum: \n",
    "            if not hasattr(layer, 'weight_momentums'): \n",
    "                layer.weight_momentums = np.zeros_like(layer.weights) \n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights \n",
    "            layer.weight_momentums = weight_updates \n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases \n",
    "            layer.bias_momentums = bias_updates\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers=[], optimizer=None):\n",
    "        self.layers = layers\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward_propagation(self, x_batch, y_batch):\n",
    "        prev_layer = None\n",
    "        for layer in self.layers:\n",
    "            layer_input = x_batch if prev_layer is None else prev_layer.output\n",
    "            if isinstance(layer, ActivationSoftmaxLossCategoricalCrossentropy):\n",
    "                loss = layer.forward(layer_input, y_batch)\n",
    "            else:\n",
    "                layer.forward(layer_input)\n",
    "            prev_layer = layer\n",
    "        return loss\n",
    "    \n",
    "    def back_propagation(self, y_batch):\n",
    "        layers_reversed = self.layers[::-1]\n",
    "        prev_layer = None\n",
    "        adjustable_layers = (LayerDense, LayerConvolutional)\n",
    "        for layer in layers_reversed:\n",
    "            if isinstance(layer, ActivationSoftmaxLossCategoricalCrossentropy):\n",
    "                layer.backward(layer.output, y_batch)\n",
    "            else:\n",
    "                layer.backward(prev_layer.dinputs)\n",
    "            prev_layer = layer\n",
    "        self.optimizer.pre_update_params()\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, adjustable_layers):\n",
    "                self.optimizer.update_params(layer)\n",
    "        self.optimizer.post_update_params()\n",
    "    \n",
    "    def train(self, x, y, epochs, batch_size):\n",
    "        for i in range(epochs):\n",
    "            for j in range(0, len(x), batch_size):\n",
    "                x_batch = x[j:j+batch_size]\n",
    "                y_batch = y[j:j+batch_size]\n",
    "                loss = self.forward_propagation(x_batch, y_batch)\n",
    "                self.back_propagation(y_batch)\n",
    "                predictions = np.argmax(self.layers[-1].output, axis=1)\n",
    "                accuracy = np.mean(predictions == np.argmax(y_batch, axis=1))\n",
    "            print(f'epoch: {i}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}')\n",
    "    \n",
    "    def save_to(self, directory):\n",
    "        relevant_fields = set(['weights', 'biases', 'kernel_size', 'num_kernels', 'num_channels', 'stride', 'n_inputs', 'n_neurons'])\n",
    "        network_dict = {'network':{}, 'optimizer':{}}\n",
    "        for count, layer in enumerate(self.layers):\n",
    "            network_dict['network'][f'{layer.__class__.__name__}-{count}'] = {}\n",
    "            curr_layer_dict = network_dict['network'][f'{layer.__class__.__name__}-{count}']\n",
    "            for field in vars(layer):\n",
    "                if field in relevant_fields:\n",
    "                    if isinstance(layer[field], np.ndarray):\n",
    "                        curr_layer_dict[field] = layer[field].tolist()\n",
    "                    else:\n",
    "                        curr_layer_dict[field] = layer[field]\n",
    "        network_dict['optimizer'][f'{self.optimizer.__class__.__name__}'] = {}\n",
    "        for optimizer_param in vars(self.optimizer):\n",
    "            network_dict['optimizer'][f'{self.optimizer.__class__.__name__}'][optimizer_param] = self.optimizer[optimizer_param]\n",
    "        \n",
    "        jsonified_dict = json.dumps(network_dict, indent=4)\n",
    "        with open(directory, 'w') as fw:\n",
    "            fw.write(jsonified_dict)\n",
    "        \n",
    "            \n",
    "    \n",
    "    def load_from(self, directory):\n",
    "        with open(directory, 'r') as rf:\n",
    "            network_dict = json.load(rf)\n",
    "        \n",
    "        layers = network_dict['network']\n",
    "        optimizer_name = list(network_dict['optimizer'].keys())[0]\n",
    "        \n",
    "        if optimizer_name == 'OptimizerSGD':\n",
    "            self.optimizer = OptimizerSGD()\n",
    "        else:\n",
    "            raise Exception('Not a currently supported optimizer')\n",
    "        \n",
    "        for optimizer_param in network_dict['optimizer'][optimizer_name]:\n",
    "            self.optimizer[optimizer_param] = optimizer_param\n",
    "        \n",
    "        for layer_name in layers:\n",
    "            layer_params = layers[layer_name]\n",
    "            if layer_name.startswith('LayerDense'):\n",
    "                layer = LayerDense(layer_params['n_inputs'], layer_params['n_neurons'])\n",
    "            elif layer_name.startswith('LayerConvolutional'):\n",
    "                layer = LayerConvolutional(layer_params['num_kernels'], layer_params['num_channels'], layer_params['kernel_size'])\n",
    "            elif layer_name.startswith('LayerFlatten'):\n",
    "                layer = LayerFlatten()\n",
    "            elif layer_name.startswith('LayerMaxPooling'):\n",
    "                layer = LayerMaxPooling()\n",
    "            elif layer_name.startswith('ActivationReLU'):\n",
    "                layer = ActivationReLU()\n",
    "            elif layer_name.startswith('ActivationSoftmaxLossCategoricalCrossentropy'):\n",
    "                layer = ActivationSoftmaxLossCategoricalCrossentropy()\n",
    "            else:\n",
    "                raise Exception('Not a currently supported layer type')\n",
    "            \n",
    "            for layer_param in layer_params:\n",
    "                if isinstance(layer_params[layer_param], list):\n",
    "                    layer[layer_param] = np.asarray(layer_params[layer_param])\n",
    "                else:\n",
    "                    layer[layer_param] = layer_params[layer_param]\n",
    "            self.layers.append(layer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:28:01.525180Z",
     "start_time": "2024-04-02T19:28:01.508520Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Model - Dense layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x, num_inputs = read_mnist_images('../mnist/train-images.idx3-ubyte')\n",
    "x = x.reshape(-1, 784) / 255.\n",
    "y = one_hot_encode(read_mnist_labels('../mnist/train-labels.idx1-ubyte'), 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:28:11.869143Z",
     "start_time": "2024-04-02T19:28:11.787481Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.933, loss: 12.326\n",
      "epoch: 1, acc: 0.967, loss: 8.724\n",
      "epoch: 2, acc: 0.967, loss: 7.285\n",
      "epoch: 3, acc: 0.967, loss: 6.950\n",
      "epoch: 4, acc: 0.983, loss: 6.791\n",
      "epoch: 5, acc: 0.950, loss: 6.874\n",
      "epoch: 6, acc: 0.950, loss: 7.065\n",
      "epoch: 7, acc: 0.950, loss: 7.067\n",
      "epoch: 8, acc: 0.950, loss: 6.838\n",
      "epoch: 9, acc: 0.950, loss: 6.464\n",
      "epoch: 10, acc: 0.950, loss: 6.182\n",
      "epoch: 11, acc: 0.967, loss: 6.111\n",
      "epoch: 12, acc: 0.967, loss: 6.060\n",
      "epoch: 13, acc: 0.967, loss: 5.972\n",
      "epoch: 14, acc: 0.967, loss: 5.883\n",
      "epoch: 15, acc: 0.967, loss: 5.913\n",
      "epoch: 16, acc: 0.967, loss: 5.873\n",
      "epoch: 17, acc: 0.967, loss: 5.894\n",
      "epoch: 18, acc: 0.967, loss: 5.823\n",
      "epoch: 19, acc: 0.967, loss: 5.729\n"
     ]
    }
   ],
   "source": [
    "# train the network yourself\n",
    "network_mnist_dense = Network([\n",
    "    LayerDense(784, 10),\n",
    "    ActivationReLU(),\n",
    "    LayerDense(10, 10),\n",
    "    ActivationSoftmaxLossCategoricalCrossentropy()\n",
    "], OptimizerSGD())\n",
    "epochs = 20\n",
    "batch_size = 60\n",
    "network_mnist_dense.train(x, y, epochs, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:29:51.265021Z",
     "start_time": "2024-04-02T19:29:23.284227Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load network from a file\n",
    "network_mnist_dense = Network()\n",
    "network_mnist_dense.load_from('../saved_models/mnist_dense.json')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save the network to a file\n",
    "network_mnist_dense.save_to('../saved_models/mnist_dense.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:31:32.700250Z",
     "start_time": "2024-04-02T19:31:32.685494Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on eval dataset: 93.01%\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "x_test, num_inputs = read_mnist_images('../mnist/t10k-images.idx3-ubyte')\n",
    "y_test = one_hot_encode(read_mnist_labels('../mnist/t10k-labels.idx1-ubyte'), 10)\n",
    "x_test_flat = x_test.reshape(-1, 784) / 255.\n",
    "for test_sample_num in range(num_inputs):\n",
    "    network_mnist_dense.forward_propagation(x_test_flat[test_sample_num:test_sample_num+1, :], y_test[test_sample_num:test_sample_num+1])\n",
    "    pred = np.argmax(network_mnist_dense.layers[-1].output)\n",
    "    correct = np.argmax(y_test[test_sample_num])\n",
    "    if pred != correct:\n",
    "        count += 1\n",
    "print(f'Accuracy on eval dataset: {100 - count/num_inputs * 100}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:37:27.466624Z",
     "start_time": "2024-04-02T19:37:27.228554Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fashion MNIST model - Dense layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_fashion, num_inputs_fashion = read_mnist_images('../fashion_mnist/train-images-idx3-ubyte')\n",
    "x_fashion_flat = x_fashion.reshape(-1, 784) / 255.\n",
    "y_fashion = one_hot_encode(read_mnist_labels('../fashion_mnist/train-labels-idx1-ubyte'), 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:32:02.767949Z",
     "start_time": "2024-04-02T19:32:02.688903Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.700, loss: 10.211\n",
      "epoch: 1, acc: 0.700, loss: 7.105\n",
      "epoch: 2, acc: 0.700, loss: 6.009\n",
      "epoch: 3, acc: 0.700, loss: 5.568\n",
      "epoch: 4, acc: 0.700, loss: 5.217\n",
      "epoch: 5, acc: 0.700, loss: 4.836\n",
      "epoch: 6, acc: 0.700, loss: 4.591\n",
      "epoch: 7, acc: 0.700, loss: 4.415\n",
      "epoch: 8, acc: 0.700, loss: 4.259\n",
      "epoch: 9, acc: 0.700, loss: 4.121\n",
      "epoch: 10, acc: 0.800, loss: 4.016\n",
      "epoch: 11, acc: 0.900, loss: 3.943\n",
      "epoch: 12, acc: 0.900, loss: 3.776\n",
      "epoch: 13, acc: 0.900, loss: 3.654\n",
      "epoch: 14, acc: 0.900, loss: 3.524\n",
      "epoch: 15, acc: 0.900, loss: 3.398\n",
      "epoch: 16, acc: 0.900, loss: 3.284\n",
      "epoch: 17, acc: 0.900, loss: 3.202\n",
      "epoch: 18, acc: 0.900, loss: 3.135\n",
      "epoch: 19, acc: 0.900, loss: 3.079\n",
      "epoch: 20, acc: 0.900, loss: 3.020\n",
      "epoch: 21, acc: 1.000, loss: 2.976\n",
      "epoch: 22, acc: 1.000, loss: 2.944\n",
      "epoch: 23, acc: 1.000, loss: 2.908\n",
      "epoch: 24, acc: 1.000, loss: 2.875\n",
      "epoch: 25, acc: 1.000, loss: 2.842\n",
      "epoch: 26, acc: 1.000, loss: 2.815\n",
      "epoch: 27, acc: 1.000, loss: 2.789\n",
      "epoch: 28, acc: 1.000, loss: 2.767\n",
      "epoch: 29, acc: 1.000, loss: 2.748\n"
     ]
    }
   ],
   "source": [
    "# train the network yourself\n",
    "network_fashion_dense = Network([\n",
    "    LayerDense(784, 10),\n",
    "    ActivationReLU(),\n",
    "    LayerDense(10, 10),\n",
    "    ActivationReLU(),\n",
    "    LayerDense(10, 10),\n",
    "    ActivationSoftmaxLossCategoricalCrossentropy()\n",
    "], OptimizerSGD(learning_rate=0.01))\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "\n",
    "network_fashion_dense.train(x_fashion_flat, y_fashion, epochs, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:32:45.728148Z",
     "start_time": "2024-04-02T19:32:30.122675Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load network from file\n",
    "network_fashion_dense = Network()\n",
    "network_fashion_dense.load_from('../saved_models/fashion_dense.json')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save network to file\n",
    "network_fashion_dense.save_to('../saved_models/fashion_dense.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:38:15.433732Z",
     "start_time": "2024-04-02T19:38:15.424344Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fashion_interpretation_dict = {\n",
    "    0: 'T-shirt/top',\n",
    "    1:'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:35:38.641173Z",
     "start_time": "2024-04-02T19:35:38.637441Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Prediction: 3 - Dress\n",
      "Correct Prediction: 3 - Dress\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x15c9c9350>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specific Sample Testing\n",
    "test_sample_num = 301\n",
    "x_test_fashion, num_inputs_test_fashion = read_mnist_images('../fashion_mnist/t10k-images-idx3-ubyte')\n",
    "x_test_fashion_flat = x_test_fashion.reshape(-1, 784) / 255.\n",
    "y_test_fashion = one_hot_encode(read_mnist_labels('../fashion_mnist/t10k-labels-idx1-ubyte'), 10)\n",
    "network_fashion_dense.forward_propagation(x_test_fashion_flat[test_sample_num:test_sample_num+1, :], y_test_fashion)\n",
    "print(f'Network Prediction: {np.argmax(network_fashion_dense.layers[-1].output)} - {fashion_interpretation_dict[np.argmax(network_fashion_dense.layers[-1].output)]}')\n",
    "print(f'Correct Prediction: {np.argmax(y_test_fashion[test_sample_num])} - {fashion_interpretation_dict[np.argmax(y_test_fashion[test_sample_num])]}')\n",
    "plt.imshow(x_test_fashion[test_sample_num], cmap='gray')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:36:23.974859Z",
     "start_time": "2024-04-02T19:36:23.887080Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on eval dataset: 81.24000000000001%\n"
     ]
    }
   ],
   "source": [
    "# General Evaluation\n",
    "count = 0\n",
    "for test_sample_num in range(num_inputs_test_fashion):\n",
    "    network_fashion_dense.forward_propagation(x_test_fashion_flat[test_sample_num:test_sample_num+1, :], y_test_fashion[test_sample_num:test_sample_num+1])\n",
    "    pred = np.argmax(network_fashion_dense.layers[-1].output)\n",
    "    correct = np.argmax(y_test_fashion[test_sample_num])\n",
    "    if pred != correct:\n",
    "        count += 1\n",
    "print(f'Accuracy on eval dataset: {100 - count/num_inputs * 100}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:37:36.204185Z",
     "start_time": "2024-04-02T19:37:35.955131Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fashion MNIST model - CNN Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_fashion_image_reshaped = x_fashion.reshape(-1, 1, 28, 28) / 255."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:38:40.301371Z",
     "start_time": "2024-04-02T19:38:40.177341Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.900, loss: 5.808\n",
      "epoch: 1, acc: 0.900, loss: 5.000\n",
      "epoch: 2, acc: 0.900, loss: 4.624\n",
      "epoch: 3, acc: 0.900, loss: 4.365\n",
      "epoch: 4, acc: 0.900, loss: 4.214\n",
      "epoch: 5, acc: 0.900, loss: 4.132\n",
      "epoch: 6, acc: 0.900, loss: 4.061\n",
      "epoch: 7, acc: 0.900, loss: 4.022\n",
      "epoch: 8, acc: 0.900, loss: 3.992\n",
      "epoch: 9, acc: 0.900, loss: 3.968\n",
      "epoch: 10, acc: 0.900, loss: 3.939\n",
      "epoch: 11, acc: 0.800, loss: 3.918\n",
      "epoch: 12, acc: 0.800, loss: 3.896\n",
      "epoch: 13, acc: 0.800, loss: 3.870\n",
      "epoch: 14, acc: 0.800, loss: 3.837\n",
      "epoch: 15, acc: 0.800, loss: 3.791\n",
      "epoch: 16, acc: 0.800, loss: 3.743\n",
      "epoch: 17, acc: 0.800, loss: 3.694\n",
      "epoch: 18, acc: 0.800, loss: 3.645\n",
      "epoch: 19, acc: 0.800, loss: 3.606\n",
      "epoch: 20, acc: 0.800, loss: 3.568\n",
      "epoch: 21, acc: 0.800, loss: 3.536\n",
      "epoch: 22, acc: 0.800, loss: 3.504\n",
      "epoch: 23, acc: 0.800, loss: 3.470\n",
      "epoch: 24, acc: 0.800, loss: 3.437\n"
     ]
    }
   ],
   "source": [
    "# train the network yourself (TRAINING THIS NETWORK IS EXTREMELY SLOW)\n",
    "network_fashion_cnn = Network([\n",
    "    LayerConvolutional(2, 1, 3),\n",
    "    LayerMaxPooling(),\n",
    "    LayerConvolutional(3, 2, 3),\n",
    "    LayerMaxPooling(),\n",
    "    LayerFlatten(),\n",
    "    LayerDense(147, 10),\n",
    "    ActivationReLU(),\n",
    "    LayerDense(10, 10),\n",
    "    ActivationSoftmaxLossCategoricalCrossentropy()\n",
    "], OptimizerSGD(learning_rate=0.01))\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 10\n",
    "\n",
    "network_fashion_cnn.train(x_fashion_image_reshaped, y_fashion, epochs, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T21:06:00.204056Z",
     "start_time": "2024-04-02T20:30:47.398690Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load network from file\n",
    "network_fashion_cnn = Network()\n",
    "network_fashion_cnn.load_from('../saved_models/fashion_cnn.json')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save network to file\n",
    "network_fashion_cnn.save_to('../saved_models/fashion_cnn.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T21:06:33.670920Z",
     "start_time": "2024-04-02T21:06:33.664054Z"
    }
   },
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on eval dataset: 80.57%\n"
     ]
    }
   ],
   "source": [
    "# General Evaluation\n",
    "count = 0\n",
    "x_test_fashion_reshaped = x_test_fashion.reshape(-1, 1, 28, 28) / 255.\n",
    "for test_sample_num in range(num_inputs_test_fashion):\n",
    "    network_fashion_cnn.forward_propagation(x_test_fashion_reshaped[test_sample_num:test_sample_num+1, :, :, :], y_test_fashion[test_sample_num:test_sample_num+1])\n",
    "    pred = np.argmax(network_fashion_cnn.layers[-1].output)\n",
    "    correct = np.argmax(y_test_fashion[test_sample_num])\n",
    "    if pred != correct:\n",
    "        count += 1\n",
    "print(f'Accuracy on eval dataset: {100 - count/num_inputs * 100}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T21:06:28.433741Z",
     "start_time": "2024-04-02T21:06:23.981181Z"
    }
   },
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Prediction: 1 - Trouser\n",
      "Correct Prediction: 1 - Trouser\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x175f65a50>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specific Sample Testing\n",
    "test_sample_num = 345\n",
    "network_fashion_cnn.forward_propagation(x_test_fashion_reshaped[test_sample_num:test_sample_num+1, :, :, :], y_test_fashion)\n",
    "print(f'Network Prediction: {np.argmax(network_fashion_cnn.layers[-1].output)} - {fashion_interpretation_dict[np.argmax(network_fashion_cnn.layers[-1].output)]}')\n",
    "print(f'Correct Prediction: {np.argmax(y_test_fashion[test_sample_num])} - {fashion_interpretation_dict[np.argmax(y_test_fashion[test_sample_num])]}')\n",
    "plt.imshow(x_test_fashion_reshaped[test_sample_num, 0], cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T21:06:50.569480Z",
     "start_time": "2024-04-02T21:06:50.488824Z"
    }
   },
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
